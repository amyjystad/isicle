import isicle
import pickle
import glob
import numpy as np
from os.path import *
import pandas as pd
import os
from rdkit import Chem

configfile: 'config.yaml'

# infer wildcards from inputs
fns = [basename(x) for x in glob.glob(join('npmrd_input','0009', '*.*'))]
IDS = [splitext(splitext(x)[0])[0] for x in fns]
lookup = {k: v for k, v in zip(IDS, fns)}

rule all:
    input:
        expand(join('output', 'dft', '{id}', '{conformer}.pkl'),
               id=IDS, conformer=1)

checkpoint conformers:
    input:
        lambda wildcards: join('npmrd_input', '0009', lookup[wildcards.id.split('/')[0]])
    output:
        join('output', 'conformers', '{id}', '1.pkl')
    benchmark:
        join('output', 'benchmark', 'conformers', '{id}.benchmark')
    resources:
        name =	 'conformers',
        runtime = 600,
        nodes = 1,
        ntasks = 10,
        partition = 'shared,slurm'
    run:
        geom = isicle.geometry.load(input[0])
        charge = geom.get_formal_charge()
        conformers = geom.md(forcefield=config['md']['forcefield'],
                            ewin=config['md']['energywindow'],
                            task=config['md']['task'],
                            charge=charge,
                            processes=resources.ntasks)

        dir_path = os.path.dirname(output[0])
        if not os.path.exists(dir_path):
            os.mkdir(dir_path)
        for conformer in conformers.geom:
            conformer.set_formal_charge(charge)
            output_path = join(dir_path, '{}.pkl'.format(conformer.conformerID))
            with open(output_path, 'wb+') as f:
                pickle.dump(conformer, f)


checkpoint dft:
    input:
        join('output', 'conformers', '{id}', '1.pkl')
    output:
        join('output', 'dft', '{id}', '1.pkl')
    benchmark:
        join('output', 'benchmark', 'dft', '{id}.benchmark')
    resources:
        name = 'dft',
        runtime = 2400,
        nodes = 1,
        ntasks = 4,
        partition = 'slurm'
    run:
        with open(input[0], 'rb') as f:
            geom = pickle.load(f)


        dft = isicle.qm.NWChemWrapper()

        dft.set_geometry(geom)
        dft.save_geometry()
        dft.configure(tasks=config['dft']['tasks'],
                      functional=config['dft']['functional'],
                      basis_set=config['dft']['basis_set'],
                      ao_basis=config['dft']['ao_basis'],
                      charge=geom.charge,
                      atoms=config['dft']['atoms'],
                      temp=config['dft']['temp'],
                      cosmo=config['dft']['cosmo'],
                      solvent=config['dft']['solvent'],
                      gas=config['dft']['gas'],
                      max_iter=config['dft']['max_iter'],
                      mem_global=config['dft']['mem_global'],
                      mem_heap=config['dft']['mem_heap'],
                      mem_stack=config['dft']['mem_stack'],
                      scratch_dir=config['dft']['scratch_dir'],
                      processes=resources.ntasks,
                      command=config['dft']['command'])
        print(dft.__dict__)
        dft.submit()
        dft.finish()


        print(output[0].rsplit('/')[0])
        if not os.path.exists(output[0].rsplit('/')[0]):
            os.mkdir(output[0].rsplit('/')[0])

        with open(output[0], 'wb+') as f:
            pickle.dump(dft, f)

rule finalize:
    input:
        join('output', 'dft', '{id}', '{conformer}.pkl')
    output:
        join('output', 'finalize', '{id}', '{conformer}.pkl')
    benchmark:
        join('output', 'benchmark', 'finalize', '{id}', '{conformer}.benchmark')
    resources:
        name = 'finalize',
        runtime = 60,
        nodes = 1,
        ntasks = 2,
        partition = 'shared,slurm,short'
    run:
        """
        cp -f {input} {output}
        """

def aggregate_input(wildcards):
    output_conf = glob.glob(
        f"{checkpoints.conformers.get(id=wildcards.id).output}/*.pkl")
    output_conf = [output.split('/')[-1].split('.')[0] for output in output_conf]
    split_files = []
    for conf in output_conf:
        output_dft = glob.glob(
            f"{checkpoints.dft.get(id=wildcards.id, conformer=conf)}.pkl")
        split_files.append(
            f"output/finalize/{wildcards.id}/{conf}.pkl")
    return split_files


rule aggregate_output:
    input:
        aggregate_input
    output:
        join('output', 'compiled', '{id}.pkl')
    benchmark:
        join('output', 'benchmark', 'compiled', '{id}.benchmark')
    resources:
        name = 'adduct',
        runtime = 60,
        nodes = 1,
        ntasks = 2,
        partition = 'shared,slurm,short'
    run:
        output_path = 'output/compiled/{}.pkl'.format(wildcards.id)
        #if not os.path.exists(dir_path):
        #    os.mkdir(dir_path)
        all_dft = []
        for fi in input:
            with open(fi, 'rb') as f:
                contents = pickle.load(f)
            all_dft.append(contents)
        with open(output_path, 'wb+') as f:
            pickle.dump(all_dft, f)
